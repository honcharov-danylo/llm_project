{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fd5d49-c91d-4bcd-a979-14a475ef76e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "\n",
    "from peft import PeftModel\n",
    "DEVICE = \"mps\" # ← adjust to \"cpu\", \"cuda\" etc. (I tested cpu, cuda, mps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8212414",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\", device_map=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc2b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(base, \"models/finetuned_smaller\").to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/finetuned_smaller\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d575cd3c-ce24-45c4-b258-3efcf399361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
    "Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "You are a scientist with advanced knowledge in philosophy.\n",
    "Write the next paragraph for the following text.\n",
    "\n",
    "### Text:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e20c83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt_style = \"\"\"\n",
    "You are a professor of philosohy.\n",
    "You are helpful, honest, and harmless.\n",
    "Below is a question. Write an answer.\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb0e961-b267-4ae2-acfa-f5ae2e92465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'What is moral reasoning?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25c08fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b455d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display \n",
    "import ipywidgets as widgets\n",
    "\n",
    "class NotebookStreamer(TextStreamer):\n",
    "    def __init__(self, tokenizer, **kwargs):\n",
    "        super().__init__(tokenizer, skip_prompt=True, **kwargs)\n",
    "        self.output = widgets.HTML(\"<pre style='white-space: pre-wrap; word-break: break-word; font-family: monospace'></pre>\")\n",
    "        display(self.output)\n",
    "        self.buffer = \"\"\n",
    "\n",
    "    def on_finalized_text(self, text, stream_end=False):\n",
    "        self.buffer += text\n",
    "        self.output.value = f\"<pre style='white-space: pre-wrap; word-break: break-word; font-family: monospace'>{self.buffer}</pre>\"\n",
    "\n",
    "streamer = NotebookStreamer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6121e1c-15aa-41fc-ada4-fcda955217b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    [test_prompt_style.format(question) + tokenizer.eos_token],\n",
    "    return_tensors=\"pt\"\n",
    ").to(DEVICE)\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    "    do_sample=True,           # ← enable sampling\n",
    "    temperature=0.7,          # ← randomness control, 0.7–1.0 works well\n",
    "    top_k=50,                 # ← limit sampling to top 50 tokens\n",
    "    top_p=0.9,                # ← or use nucleus sampling\n",
    "    repetition_penalty=1.2,   # ← discourage loops and repeats\n",
    "    streamer=streamer,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(response[0].split(\"### Response:\")[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
